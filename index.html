<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Grasp it Like a Pro 2.0"/>
  <meta property="og:description" content="Grasp it Like a Pro 2.0: A Data-Driven Approach Exploiting Basic Shapes Decomposition and Human Data for Grasping Unknown Objects"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Grasp it Like a Pro 2.0: A Data-Driven Approach Exploiting Basic Shapes Decomposition and Human Data for Grasping Unknown Objects	</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Grasp it Like a Pro 2.0: A Data-Driven Approach Exploiting Basic Shapes Decomposition and Human Data for Grasping Unknown Objects	</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://alessandropalleschi.github.io" target="_blank">Alessandro Palleschi<sup>*</sup></a>,</span>
                <span class="author-block">
                  <a href="https://www.centropiaggio.unipi.it/~angelini" target="_blank">Franco Angelini<sup>*</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://people.utwente.nl/c.gabellieri" target="_blank">Chiara Gabellieri<sup>+</sup></a>,
                  </span>
                    <span class="author-block">
                      <a href="" target="_blank">Do Won Park<sup>*</sup></a>,
                    </span>
                    <span class="author-block">
                      <a href="https://www.centropiaggio.unipi.it/~pallottino" target="_blank">Lucia Pallottino<sup>*</sup></a>,
                    </span>
                    <span class="author-block">
                      <a href="https://www.centropiaggio.unipi.it/~bicchi" target="_blank">Antonio Bicchi<sup>*</sup></a>,
                    </span>  
                    <span class="author-block">
                      <a href="https://www.centropiaggio.unipi.it/~garabini" target="_blank">Manolo Garabini<sup>*</sup></a>
                    </span>  

                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>*</sup>Research Center "Enrico Piaggio", University of Pisa, <sup>+</sup>Robotics and Mechatronics Lab, EEMCS Faculty, University of
Twente<br>IEEE Transactions on Robotics (Under Review)</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.dropbox.com/s/lhdfjomooxyqtss/bare_jrnl_new_sample4.pdf?dl=0" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                               <span class="link-block">
                  <a href="#BibTeX" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-graduation-cap"></i>
                                        </span>
                    <span>Cite</span>
                  </a>
                </span>

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
 -->
                  <!-- Github link -->
<!--                   <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
 -->
                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The ability to grasp previously unseen objects with different
        grippers, adapting to imperfectly known, highly dynamic, and
        unstructured situations is crucial to enable general-purpose robots
        to be effective in a large field of use cases.      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With the improvements in the
            computational and physical intelligence of robots,
            they are now capable of operating in real-world
            environments. However, manipulation and grasping
            capabilities are still areas that require significant
            improvements. To address this, we propose a novel
            data-driven grasp planning algorithm called <strong>Grasp it
            Like a Pro 2.0</strong>. <br> Our algorithm leverages a small set
            of human demonstrations to transfer the skills of a
            human operator to a robot for grasping of previously
            unseen objects. By decomposing objects into basic
            shapes, our algorithm generates candidate grasps
            that can generalize to different object’s geometry.
            The algorithm selects the grasp to execute based
            on a selection policy that maximizes a novel grasp
            quality metric introduced in this work. This metric
            considers the complex interdependencies between the
            predicted grasp, the local approximation produced
            by the basic shape decomposition, and the gripper
            used for the grasp to rank the generated candidate
            grasps. <br> We compare the method with several baselines
            with different grippers and unknown objects. Results
            demonstrate that our method can generate and select
            high-quality and robust grasps, achieving a success
            rate of 94.0% on 150 grasps of 30 different objects
            with a soft underactuated robotic hand, and an 85.0%
            success rate on 80 grasps of 16 different objects with a
            rigid gripper.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" width="auto"
        height="auto"/>
        <h2 class="subtitle has-text-centered">
          Pipeline of the proposed method Grasp it Like a Pro 2.0. GLP 2.0 starts with the acquisition of a point cloud of the target object.
          The cloud is decomposed into a generic number N of minimum volume bounding boxes. A model learned from human demonstrations of
          grasps for exemplary boxes is used to generate a set of 6-DoF grasp poses for the obtained decomposition. A novel grasp quality score, Sg, is
          introduced based on information about the robotic gripper, the point cloud of the object and the environment, and of the grasp interaction
          forces estimated using a learned model to rank and select the best grasp among the candidate set.        </h2>
      </div>
      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT" class="center" width="550"/>
        <h2 class="subtitle has-text-centered">
          Experimental setup used to record the set of human
          demonstrations. The PhaseSpace cameras (1) are used to track the
          pose of the hand using eight hand-fixed markers (5) and eight boxfixed
          markers (7). A human operator (4) grasps a set of boxes (2)
          with the Pisa/IIT SoftHand (6). A force torque-sensor (8) is used to
          record the interaction wrenches. The recorded data are saved on a
          PC (3), and later used to train a DTR model. Training on a small
          set of recorded demonstrations of a skilled operator grasping cuboids,
          the robot is able to generate a grasp pose for a generic object.        </h2>
      </div>
      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Frames of a Franka Emika Panda manipulator grasping objects with the Pisa/IIT SoftHand and the Franka Hand.
       </h2>
     </div>

      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Results of the comparison with GPD and CS-GQ-CNN using the Pisa/IIT SoftHand. GLP 2.0 outperforms both baselines on the
          tested objects, relying on a smaller set of demonstrations.
       </h2>
     </div>
     <div class="item" style="text-align: center;">
      <!-- Your image here -->
      <img src="static/images/carousel5.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Grasping success rate using the Franka Hand. GLP 2.0 achieves an overall grasping success rate of 85.0%,
comparable to the one obtained on the same objects with the Pisa/IIT SoftHand (91.25%). GLP 2.0 outperforms the other two baselines,
which score a success rate lower than 60%.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <video poster="" id="tree" autoplay="false" controls muted loop height="100%">
              <!-- Your video here -->
              <source src="static/videos/multimedia.mp4"
              type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
 -->            <!-- Your video file here -->
<!--             <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
 -->            <!-- Your video file here -->
<!--             <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
 -->            <!-- Your video file here -->
<!--             <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
 --><!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="static/pdfs/paper.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{palleschi2023grasp,
        title={Grasp it Like a Pro 2.0: A Data-Driven Approach Exploiting Basic Shapes Decomposition and Human Data for Grasping Unknown Objects},
        author={Palleschi, Alessandro and Angelini, Franco and Gabellieri, Chiara and Park, Do Won and Pallottino, Lucia and Bicchi, Antonio and Garabini, Manolo},
        journal=IEEE Transactions on Robotics,
        year={Under Review},
        publisher={IEEE}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <div class="content has-text-justified">
      <p>  
        This work was supported in part by the European Union’s Horizon
2020 Research and Innovation Program under Grant Agreements
No. 871237 (Sophia) and No. 101017274 (DARKO), in part by the
Ministry of University and Research (MUR) as a part of the PON
2014-2021 “Research and Innovation” resources—Green/Innovation
Action—DM MUR 1062/2021, and by the Italian Ministry of
Education and Research in the framework of the CrossLab and
FoReLab projects (Departments of Excellence).
      </div>
  </div>
</section>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
